제목: '불가해성'은 인공지능 진화의 본질
날짜: 20170419
기자: 김은광
링크: https://www.bigkinds.or.kr/news/detailView.do?docId=01100301.20170419124100001
ID: 01100301.20170419124100001
카테고리: IT_과학>과학
본문: ▶18일자 13면 '정신분열증까지 예측하는 인공지능, 인간은 그 과정을 이해하지 못한다'에서 이어집니다. <br/> <br/>'기계의 자율학습' 개념은 애초 실용성과 가능성 측면에서 큰 제약이 있었다. 때문에 60년대와 70년대 인공지능 연구에서는 변방으로 밀렸다. 하지만 대부분 산업계가 컴퓨터를 이용하게 되고 이를 기반으로 빅데이터가 방대하게 축적되면서 이 접근법은 다시 주목받기 시작했다. 1990년대 '인공지능망'으로 알려진 새로운 버전의 기계 자율학습 기술이 개발되면서 전 세계 과학자들은 너나없이 연구에 매달렸다. 그 결과 손글씨도 0과 1의 숫자로 디지털화되는 세상이 열렸다. <br/> <br/>하지만 연구성과가 본격화한 것은 2010년대 들어서다. 인공지능망이 딥러닝을 통해 자동인지 기능을 갖게 되면서다. 딥러닝은 AI의 폭발적 확산을 불렀다. 사람의 말을 스스로 인식하는 등 전례없이 막강한 힘이 컴퓨터에 부여됐다. 사람이 수작업으로 코드를 입력해서는 하늘과 땅이 뒤바뀌어도 불가능한 일을 기계 스스로 해내는 것이다. 이제 AI는 의학과 금융, 제조업을 비롯한 산업 전 분야를 뒤집으려 하고 있다. <br/> 구글이 만든 기계화가 '딥드림'이 그린 그림. 기괴하고 음산한 분위기를 풍긴다. 실재와 유사한 이미지를 만들어내면서도 인간과 달리 부차적 정보를 중시하는 특성을 보였다. 사진출처 구글포토 딥러닝 기술은 본질적으로 '불투명성'을 특징으로 한다. 모든 기록을 다 담고 있지만 인간이 이를 열어 해독할 수는 없는 '어두운 블랙박스'(Dark Black Box)다. 이를 개발한 컴퓨터공학자들도 모르긴 마찬가지다. 앞으로 더욱 향상될 미래 AI 기술은 인간이 짐작조차 할 수 없을 것은 불을 보듯 뻔하다. <br/> <br/>작동원리를 알기 위해 인공신경망의 내부를 들여다볼 수조차 없다. 인공신경망의 추론은 수천가닥 개별 뉴런(신경세포)의 움직임에 심어져 있고, 이는 다시 복잡하게 연결된 수십개, 많으면 수백개의 층(layer)에 배열돼 있기 때문이다. 첫 번째 층에 배열된 뉴런 각각은 이미지를 구성하는 픽셀의 선명도와 같은 입력신호를 받아들인다. 그리고 나서 계산을 수행한 뒤 새로운 신호를 발산한다. 복잡한 신경망 속에서 이렇게 생성된 신호는 다음 층의 뉴런에게 전달된다. 층층마다 이같은 과정을 거쳐 최종 결과물이 생성된다. 여기에다 인공신경망이 더 나은 결과를 생성해낼 수 있도록, 개별 뉴런들의 활동을 향상시키는 복습 과정이 곁들여진다. <br/> <br/>인공신경망을 구성하는 수많은 층들은 사물을 인식할 때 각기 다른 추상화의 과정을 거친다. 예를 들어 개를 인식하는 과정에서 저차원의 층들은 개의 윤곽이나 전체 색깔 등과 같은 단순한 정보를 받아들이고, 고차원의 층들은 개의 털과 눈색깔 등 보다 복잡한 정보를 받아들인다. 최상위의 층이 전체 정보를 모아 온전한 의미로서의 개를 인식한다. 이같은 접근법은 기계가 스스로 인간의 대화를 학습하는 과정에도 적용된다. 대화 속에서 단어를 구성하는 소리를 인식하는 층이 있고, 텍스트 내에서 문장을 만드는 철자와 단어를 인식하는 층이 있다. 자율주행차의 작동도 마찬가지다. <br/> <br/>인공지능망 시스템에 어떤 일이 일어나고 있는지를 알기 위해 여러가지 전략적 시도가 이뤄졌다. 2015년 구글 개발팀은 딥러닝 기반의 이미지 인식 알고리즘을 바꿔봤다. 사진 속 사물을 인식하는 게 아니라 스스로 이미지를 만들어내고 바꾸도록 조정한 것이다. 알고리즘 변경을 통해 개발팀은 인공지능이 새나 건물 등 사물을 인식하는 특징들을 알아낼 수 있었다. <br/> <br/>'딥드림'(Deep Dream)으로 불리는 기계 화가가 만들어낸 이미지는 기괴하고 음산한 분위기를 풍긴다. 이같은 이미지는 딥러닝이 완전히 불가해한 것만은 아니라는 사실을 증명했다. 즉, 알고리즘이 실제 이미지와 유사한 것을 만들어낸다는 사실을 드러낸 것이다. <br/> <br/>하지만 인간의 인식과는 상당히 다르다는 사실도 알 수 있었다. 사람이라면 무시할 만한 곁가지 정보를 중시하는 성향을 드러낸 것. 구글 개발팀에 따르면 딥러닝 알고리즘이 '아령'이라는 이미지를 그릴 때 아령을 들고 있는 인간의 팔도 포함시켰다. 구글 AI는 부차적 정보인 사람의 팔을 아령이라는 개념 속에 포함시키고 있는 것이다. <br/> <br/>신경과학과 인지과학의 개념을 활용해 인공지능을 해부하고자 하는 노력도 있다. <br/> <br/>미 와이오밍대 제프 클룬 교수팀은 인공신경망을 실험하기 위해 시각적 환상을 이용했다. 2015년 클룬 교수팀은 특정 이미지가 인공신경망을 속여 실재하지 않은 사물을 인식하게 할 수 있다는 것을 보여줬다. 해당 이미지는 AI 시스템이 탐색하는 저차원 패턴을 활용해 인공지능을 속였다. 클룬 교수팀의 연구원인 제이슨 요신스키는 인간 두뇌에 삽입된 '외과용 탐침'과 비슷한 역할을 하는 도구를 만들었다. 이 도구를 인공신경망 내 뉴런에 접속해 뉴런을 최대치로 활성화시키는 이미지를 찾아봤다. 실험 결과 인공지능망이 떠올린 이미지들은 매우 추상적인 것이었다. AI 지각력의 미스터리한 본성을 단적으로 드러냈다. <br/> <br/>AI의 사고과정을 엿보는 것 이상의 노력이 필요한 상황이지만 말처럼 쉽지 않다. 인공지능망의 추론 과정은 상호작용의 결과물이다. 이 덕분에 인간처럼 고차원적 패턴 인식과 복잡한 의사결정이 가능하다. 하지만 그같은 추론 과정은 수학적 기능과 변수들이 복잡하게 엉킨 '늪'(quagmire)과 유사하다. MIT 교수 토미 자콜라는 "작은 규모의 신경망이라면 인간이 이해할 수 있을지 모른다"며 "하지만 규모가 커지면 하나의 층마다 수천개의 뉴런이 존재하고 그같은 층이 수백개에 이른다. 이는 인간이 이해할 수 있는 수준을 넘어선다"고 말한다. <br/> <br/>자콜라 교수실 옆방은 레기나 바질레이 교수실이다. 바질레이 교수는 딥러닝을 의학에 접목하는 데 매진하고 있다. 그는 2년 전 유방암 진단을 받았다. 자신이 암에 걸렸다는 사실 자체도 충격이었지만 최첨단 딥러닝 모델을 적용할 수 없다는 사실이 더 큰 실망이었다. 바질레이 교수는 "<span class='quot0'>AI는 의료계에 혁명을 일으킬 잠재력을 갖고 있다</span>"면서도 "<span class='quot0'>잠재력을 현실화하려면 의료기록 이상의 무언가가 필요하다</span>"고 말했다. 그는 "<span class='quot0'>현재 활용되지 못하는 이미지 데이터나 병리 데이터 등 모든 미가공자료를 사용해 연구하는 내 모습을 상상한다</span>"고 덧붙였다. <br/> <br/>지난해 암치료를 끝낸 바질레이 교수는 메사추세츠종합병원 의사들과 함께 병리학 보고서를 활용하는 AI 시스템을 개발중이다. 독특한 임상적 특징을 가진 환자들을 연구하기 위해서다. 하지만 바질레이 교수 역시 AI 시스템의 추론과정이 이해가능해야 한다는 점을 잘 알고 있다. 이를 위해 그는 자콜라 교수팀과 함께 한 가지 목표 단계를 추가했다. AI가 특정 질환의 대표적 패턴을 발견할 경우 정보의 일부를 추출해 인간에게 보여주도록 하는 것이다. 한편 그는 가슴 X레이 사진으로 유방암의 초기단계를 알아내는 딥러닝 알고리즘을 개발중이다. 그는 "<span class='quot0'>인간과 기계가 협력하기 위해서는 선순환 과정이 필요하다</span>"고 말했다. <br/> <br/>미 국방부도 기갑병과 차량이나 전투기 등의 자율주행, 적의 식별, 군사정보 생산 등의 부문에 인공지능을 활용하기 위해 수십억달러의 예산을 쓰고 있다. 국방 분야 인공지능은 그 어떤 분야보다 실수가 없어야 한다. 의료부문보다 더 세심해야 한다. 국방은 '생살여탈'을 목표로 하기 때문이다. 인공지능에 대한 인간의 이해가능성이 핵심 관건이다. <br/> <br/>국방부 산하 방위선진연구기획국(DARPA) 매니저인 데이빗 거닝은 '설명가능한 인공지능'(Explainable AI) 프로그램을 맡고 있다. DARPA의 예전 프로젝트는 애플사의 음성인식 비서인 '시리' 개발로 이어진 바 있다. 그는 "<span class='quot0'>인공지능이 국방 전 분야에 확산되고 있다</span>"고 말한다. <br/> <br/>군사정보 분석가들은 AI를 활용해 산더미같은 도감청 자료 속에서 패턴을 인식해 가치 있는 정보를 생산해내는 실험을 하고 있다. 또 자율주행 기갑차량과 전투기들이 속속 개발돼 시험을 거치고 있다. 하지만 AI 로봇탱크의 작동원리가 명쾌하게 이해되지 않는다면, 그와 함께 임무를 수행한 전투원들은 불안감을 느낄 수밖에 없다. 또 추론과정을 알 수 없는 군사정보에 의거해 임무를 수행하기도 마뜩잖은 일이다. 거닝은 "인공지능이 종종 잘못된 정보를 생성하는 건 어쩔 수 없는 부분"이라며 "AI의 정보생산 과정을 이해할 수 있도록 분석가들이 추가 노력을 기울여야 한다"고 지적했다. <br/> <br/>DARPA가 추진하는 프로젝트 중 일부는 워싱턴대 교수인 카를로스 궤스트린이 주도하고 있다. 궤스트린 교수팀은 인공지능 학습 시스템이 결과물을 낼 때 근거를 함께 제시하도록 하는 방법을 개발했다. 이 방법을 사용하면 컴퓨터는 자료뭉치에서 몇개의 사례를 추출한 뒤 이에 대한 짧막한 설명을 붙인다. 보통의 인공지능은 이메일에서 테러관련 정보를 분류하기 위해 수백만개의 이메일을 학습하고 의사결정을 내린다. 하지만 궤스트린 교수팀의 접근법을 사용하게 되면 인공지능은 메시지에서 특정 키워드를 강조한다. 이미지 인식 시스템이 가장 중요한 이미지의 해당 부분을 강조하며 추론과정에 대한 힌트를 준다는 것이다. <br/> <br/>이같은 방법의 단점은 인공지능이 제공한 정보가 언제나 단순화된다는 것이다. 이는 일부 주요 정보가 누락될 수 있음을 의미한다. 바질레이 교수팀이 맞닥뜨린 장애물과 비슷하다. 궤스트린 교수는 "<span class='quot1'>인공지능이 사람과 대화를 나누고, 어떤 결정에 대해 상세히 설명하는 그림과는 거리가 멀다</span>"며 "<span class='quot1'>완전히 이해가능한 AI를 만들겠다는 꿈은 아직도 멀리 있다</span>"고 말했다. <br/> <br/>완벽한 AI를 만들겠다는 꿈이 꼭 암진단이나 군사작전처럼 중대한 이해관계가 걸린 상황을 해결하기 위해서는 아니다. 인공지능 기술이 일반화돼 인간의 삶에 유용한 부분이 되려면, AI의 추론과정을 아는 것이 중요하기 때문이다. 애플에서 시리팀을 이끌고 있는 톰 그루버는 "<span class='quot1'>인공지능에 대한 설명가능성은 우리팀에게 최고 고려사항</span>"이라며 "<span class='quot1'>시리를 더 똑똑하고 유능한 가상비서처럼 만들기 위해서</span>"라고 말했다. 애플 AI 리서치 국장이자 카네기멜론대 교수인 루슬란 살라쿠트이노프는 인간과 인공지능의 관계를 진화시키는 핵심 요소로 설명가능성을 꼽으면서 "<span class='quot2'>그래야 인간과 기계 간의 신뢰가 형성된다</span>"고 말했다. <br/> <br/>인간행동의 많은 측면을 구체적으로 이해하기 어려운 것처럼 AI도 자신이 하는 것을 모두 설명하는 건 불가능할 것이다. 와이오밍대 제프 클룬 교수는 "<span class='quot1'>사람도 자신의 행동에 대해 합리적으로 설명한다고 하지만, 듣는 입장에서는 늘 불충분하다</span>"며 "AI도 마찬가지일 것"이라고 말했다. 그는 "<span class='quot1'>합리적 설명이 가능한 것은 부분적이라는 건 지능을 가진 생물이나 기계의 본성</span>"이라며 "<span class='quot1'>인간이나 기계나 일부 행동은 본능에 따른 것이고 또 다른 일부는 잠재의식에 따른 것이며, 그외 나머지 부분은 불가해한 것</span>"이라고 말했다. <br/> <br/>그렇다면 어떤 단계로 올라서면 인간은 AI의 판단이나 행동을 전적으로 신뢰해야 한다. 그게 싫다면 AI를 사용하지 말아야 한다. 마찬가지로 AI의 판단에는 사회적 집단지성이 포함돼야 한다. 인간사회는 서로 기대된 행동을 할 것이라는 암묵적, 명시적 계약관계 위에서 건설된다. 이는 AI 시스템을 인간의 사회적 기준을 존중하고 따르도록 디자인해야 한다는 의미다. 인간이 로봇탱크나 기타 살상용 무기를 만들려 한다면, AI의 의사결정이 인간의 윤리적 판단을 따르도록 해야 한다. <br/> <br/>미 터프츠대 교수인 대니얼 데닛은 저명한 철학자이자 인지과학자다. 그는 의식과 정신에 대한 전문가다. 데닛 교수는 최근 펴낸 '박테리아에서 바흐까지'라는 책에서 의식에 대한 사상을 집대성하고자 했다. 책에 따르면 창조자가 알지 못하는 방식으로 임무를 수행하는 시스템을 만들어내는 것이 지적피조물의 자연스런 진화 과정이다. 그는 "<span class='quot1'>문제는 우리가 이를 현명하게 만들기 위해 어느 정도까지 양보할 것인가, 우리가 인공지능에게 그리고 우리 자신에게 어떤 기준을 제시할 것인가</span>"라고 말했다. <br/> <br/>데닛 교수는 "<span class='quot1'>만약 인공지능을 사용하고 의지하려 한다면, 할수 있는 한 최대치로 인공지능이 어떻게, 왜 그같은 답변을 우리에게 주는지 이해하려 노력해야 한다</span>"고 강조했다. 그러면서 " 인공지능이 어떤 사안에 대해 인간보다 더 똑똑하게 설명할 수 없다면, 그땐 AI를 신뢰하지 말아야 한다"고 덧붙였다.
